\section{Design and Implementation}
In this section, we present linux-rtxg design and implementation.

Figure~\ref{} shows over-view of linux-rtxg.
Linux-rtxg is divided into two components that are loadable kernel module and library.
linux-rtxg library is interface of communicate between application and linux-rtxg core component(kernel module).
it is using system call that is ioctl.

The part of library included speciall method.
it is independent Nvidia interrupt raised method (iNVRM).
iNVRM is used only on the nvidia driver.
if system use nouveau driver, runtime must use part of gdev.
Gdev can happen arbitary interrupt of gpu kernel in the user-space mode, and it have no need to be independent interrupt raised method.

linux-rtxg loadable kernel module is positioned kernel-space.
Thus, module can use kernel exported function.

\subsection{GPU Scheduler}

\color{red}

\begin{itemize}
\item scheduling mechanism
\item interface
\end{itemize}
\color{black}

Linux-RTXg's scheduler function is provided RTXG API.
The basic APIs supported by Linux-RTXg are listed in Table~\ref{tab:rtx-api}.
Some APIs have arguments and others do not.
Figure~\ref{fig:} shows a sample program that is using CUDA API.
Our API is don't modificated existing CUDA API for supporting proprietary CUDA API.
However, user have to add linux-rtxg api to existing CUDA application for using linux-rtxg scheduler.

\begin{table*}[t]
\begin{center}
\caption{Basic Linux-RTXg APIs}
\label{tab:rtx-api}
\begin{tabular}{|l|p{50em}|} \hline
rtx\_gpu\_open() & To register itself to linux-RTXg, and create scheduling entity. It will must call first. \\ \hline
rtx\_gpu\_launch() & To control the GPU kernel launch timing, in other words it is scheduling entry point. It will must call before the CUDA launch API. \\ \hline
rtx\_gpu\_sync() & To wait for finishing GPU kernel execution by sleeping with TASK UNINTERRUPTIBLE status.\\ \hline
rtx\_gpu\_notify() & To register the notify command to GPU microcontroller.\\ \hline
rtx\_gpu\_close() & To release scheduling entity.\\ \hline
\end{tabular}
\end{center}
\end{table*}

Figure~\ref{fig:controlflow} shows control flow of wake-up gpu kernel on the linux-rtxg's gpu scheduler.



\subsection{GPU synchronization}
GPUのアプリケーションには共通する特性がある。
それはGPUに処理を発行してから、終了するまでの待機時間が発生することである。
待機中に同一タスクで処理を継続する例もあるが、その処理結果は同期しなければ受け取ることができないため、
必ず同期時間が発生する。
この同期時間はself-suspendingに関連する問題を発生させる。
加えて、その同期方法によってはレイテンシが大幅に増加するために、
適切な手法によって同期が行われなければならない。

GPUの同期は2つの手法がある。
一つはfenceを用いる方法。
もう一つは割込みを用いる方法である。
GPUには多くのエンジン（マイクロコントローラ）が搭載されている。
本論文では詳しいアーキテクチャはメインではないので省略するが、詳細は過去の文献\cite{timegraph,fucc}に記載しています。
このエンジンにはコンテキスト管理用、コマンド受け取り用、データ転送用などが存在している。
通常、コマンド受け取り用コントローラが受け取ったコマンドのヘッダから、
そのコマンドを使用するコントローラへと送信し、処理が行われる。
この順序はすべてFIFOで行われる。

まずFenceを用いた方式では、まず同期用に仮想アドレス空間にマップされたバッファをGPUメモリに用意する。
そして、このメモリに値をEngine経由で書き込む用にコマンドを発行する。
すると、カーネル実行とメモリ転送終了後にエンジンが値を書き込むため、
CPU側でそのマップされたメモリアドレスをポーリングしながらチェックすれば同期が可能である。
割込みを用いる方式についてもEngineの機能を利用する。

タスクはTASK\_INTERRUPTIBLE/TASK\_UNINTERRUPTIBLEにした上でschedule()を呼ぶか、
waitqueueなどを用いて上記に相当する処理を行いsuspendする。
そしてEngineから割込みを発生させるコマンドを発行し、割り込みコントローラがそれを獲得、
GPUドライバが登録したISRを立ち上げる。
ISR内では、各割込みに関するステータスをマッピングされたレジスタから読み込み、
各割込みごとに処理を行う。処理後は割込み完了フラグを書き込み、初期化する。

一般的な利用の場合、多くはfenceが用いられるが、
Gdevなどはスケジューリングにおいてあるタスク終了後、次のタスクを立ち上げる部分に割込みを用いている。
一般的にこれらはCPU側の実装の仕方によって異なる。
前者は待機するタスクの状態がTASK\_RUNNINGの時に、sched\_yield()などを用いて他のタスクへの影響を考慮しながらポーリングする場合に適している。
後者は待機するタスクの状態がTASK\_INTERRUPTIBLEかTASK\_UNINTERRUPTIBLEの時に、割込みというeventによってタスクを立ち上げて処理を継続していく。

\subsubsection{Interrupt interception}
前述した割込みはデバイスドライバ（カーネルと共にパッケージされている）によって登録されたISRがハンドルする。
Linux-RTXではスケジューリング用のワーカースレッドを立ち上げており、次に実行するタスクの選択が終わってからそのタスクの実行が終わるまでは実行停止状態で待機する。
上記SCHED\_DEADLINE時のwait queueueを用いた場合に置いても、たすくの　実行が終わり同期されるまで実行停止状態で待機する。
これらを適切に立ち上げるためには任意の割込みを獲得し、外部ISRがその割込みがどのカーネルに関連しているかを識別できる仕組みが必要である。
加えて、割込みの識別はGPUのステータス・レジスタを読み込んで行う必要があり、GPUドライバが割込みレジスタをリセットする前に、実行される必要がある。

そのため我々は、GPUに関する割込みを傍受し、我々の割込みハンドラを先に呼び出し、オリジナルの割り込みハンドラへとシーケンシャルに移行するようにする。
Linuxの割込みは分割割込みを用いており、前半部分をtop-half、後半部分をbottom-halfと呼ぶ。

gllenらの提供するklmirqdはtaskletがリアルタイム性に及ぼす問題について言及し、bottom-halfに位置するtaskletをオーバライドしている。
この手法を我々の目的のために適用しようと考えた際、bottom-halfでは
我々の割込み傍受はtop-halfをオーバーライドする。

Linuxでは、割込み番号ごとにirq\_descという割込みのパラメータを保持する構造体を持っている。
この構造体には割り込みハンドラの関数ポインタを含むirq\_actionという構造体がリストで接続されている。
irq\_descはグローバルな領域に確保されており、カーネル空間からであれば誰でも参照可能である。
Linuxのローダブルカーネルモジュールはカーネル空間で動作しているため、このirq\_descを取得でき、
Interrupt handlerの関数ポインタも取得可能である。

我々はこの取得した関数ポインタを保持し、我々の傍受用割込みハンドラを設定、コールバック関数を保持している関数ポインタから設定することで、
GPUに関する割込みの発生後、先んじて取得が可能である。ただしこの手法は当然のことながら、割込みを遅延させることにほかならないため、オーバヘッドについて綿密な評価をSec\ref{sec:eval}にて示す。

\subsubsection{Independent interrupt}
我々のこれまでの実験から、NVIDIAのClosed-source software ではコンテキスト生成時の設定によってはカーネル実行後に割り込みを発生していることがわかった。
実際にinterrupt interceptionによって盗聴した結果が\ref{fig:interception_result}である。NVIDIAのドキュメントによると、CUDAはワーカースレッドを立ち上げて、
割込みを受け取り、同期を行っている。

しかしながらこの割込みは、我々のアプローチではどのカーネル実行に関連付けされているかが区別できなかった。
したがって、ひとつのGPUへの複数のアクセスを許可した場合、割込みを利用したスケジューリングが不可能になる。

そのためここでは、ランタイムから独立した割込み機構として、独自に割込みを発生させる仕組みを実装する。
NVIDIAのクローズドソースドライバはNouveauプロジェクトのリバースエンジニアリングによる解析によって、ioctlを使ったインタフェースになっていることがわかっている。
Gdevではこの解析された情報を用いて、NVIDIAのクローズドソースドライバとオープンソースライブラリという掛け合わせでCUDAを実行できる基盤が構築されている。
本論文では、この基盤から割込みを発生させる部位のみ抽出し、スケジューリングに用いる。

本手法は大きく2つに分かれ、それぞれInitializeとNotifyと呼ぶ。
Initializeは、いわゆるコンテキストの生成に値する。Virtual Address Spaceやコマンド送信に用いるIndirect Bufferの確保、コンテキストオブジェクトの生成などを行う。
NotifyはComputeエンジンやCopyエンジンに向けて割込み発生のコマンドを送信する。

本アプローチに用いるインタフェースは公式にサポートされていないために、ベンダーによる急な仕様変更には対応できない。
しかしながら、これ以外に割込みを発生させるアプローチがなく、クローズドソースを用いた場合の限界であるといえる。

\subsection{Scheduler Integration}
Linux scheduler have various real-time scheduling policies that were SCHED\_DEADLINE, SCHED\_FIFO and SCHED\_RR.
We support all scheduling policies that was implemented by linux.
However, synchronization does not work well in a specific scheduling policy.
The problem that is synchronization by fence in the SCHED\_DEADLINE.
It problem is synchronization by fence under the SCHED\_DEADLINE.
It because, implementation of sched\_yield() cede cpu to other tasks, by to set the next deadline to current deadline and to set 0 to budget.

%LinuxのスケジューリングポリシーにはSCHED\_DEADLINE, SCHED\_FIFO, SCHED\_RR, SCHED\_NORMALなどが実装されている。
%Linux-RTXgではLinuxに既存で実装されているスケジューリングポリシー全てに対応する。
%しかし、スケジューリングポリシーによっては上記の割込みが正常に動作しないケースがある。
%それがSCHED\_DEADLINEにおいてはfenceによる同期ができない問題である。

その理由はSCHED\_DEADLINEでのsched\_yield()の実装がデッドラインを次の周期に設定しバジェットを0に設定することで優先度を低下させ、他のタスクにCPUを譲っているためである。
つまり、sched\_yield()を用いたポーリングではその周期内での実行を諦めるため、必ずデッドラインミスとなる。
しかしながら、sched\_yeild()を利用しない場合、同期待ちの間CPUを専有してしまう。
CPUの専有は非効率であり好ましくない。

そのためlinux-rtxgではSCHED\_DEADLINEのポリシーの際は割込みによってGPU処理との同期を行う。
つまり、タスクを一旦サスペンドする。
しかしサスペンド後、SCHED\_DEADLINEではスリープからの復帰時に以下式(1)を用いてスケジューリング可能性のシンプルな検証を行う。
{\scriptsize
\begin{equation}
\frac{Absolute\_Deadline - Current\_Time}{Remaining\_Runtime} > \frac{Relative\_Deadline}{Period}
\end{equation}
}
式(1)が真の時、バジェットが補充され、デッドラインが次の周期に設定される。
従って同期時に必ず自らサスペンドすることによってこの検証に引っかかり、デッドラインが更新されてしまう。
これはConstand Bandwidth Serverの仕様であり、self-suspensionを含んだタスクのスケジューリングを考慮していないためである。
このself-suspensionはスケジューリング可能性についても影響を与えており、リアルタイムマルチコアスケジューリングの困難な問題の一つである。
不幸なことだが我々の知る限り、このSelf-Suspensionはglobalリアルタイムアルゴリズムにおいて解決された例は無く、
我々もそれを完全に解決する手段は提供することができない。

我々はこの復帰時のチェックに関しては、スリープ中も順調にタスクが実行していると仮定して、スリープしている時間(=GPUの実行時間)をRemaining\_Runtimeから引くことで対応した。
システム設計者はこれを想定して、runtimeのパラメータにはCPU execution time + GPU execution time (included data transmission time)を含めて設定しなければならない制約があるため、最適ではない手法ではあるが、カーネルを操作しない手法としてはこれが最善ということで妥協した。

以上の、割込み方式での同期+復帰時のパラメータ調整によってSCHED\_DEADLINE下でのGPU実行を含むタスクをサポートした。
その他のスケジューリングポリシーではfenceによる同期でも問題は発生しないため、全スケジューリングポリシーをサポートできたといえる。

