\section{Design and Implementation}\label{sec:design_imple}
In this section, we present Linux-RTXG design and implementation.
Linux-RTXG is an abbreviation of Linux Real-Time eXtension including GPU resouce management,
which is no patched Linux real-time GPU scheduling framework.

We describe main contribution of the GPU scheduler and the integration to CPU scheduler in this paper,
while CPU scheduling description is to minimize by Linux-RTXG is base RESCH.

%Linux-RTXGはLinux Real-Time eXtension included GPU resource managementの略称であり、Linuxのリアルタイム拡張に加えてGPUのResourceマネージメントを行うためのフレームワークである。
%Linux-RTXGはベースとしてRESCHを用いているため、CPUスケジューラに関する記述は最小限に抑え、
%本論文の大筋である、GPUスケジューラをメインに、CPUスケジューラとの統合といった部分を記載していく。

\subsection{Linux-RTXG}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.35\textwidth]{img/overview.pdf}
\caption{Over view of the Linux-RTXG}
\end{center}
\label{fig:overview}
\end{figure}

Figure~\ref{fig:overview} shows over-view of Linux-RTXG.
Linux-RTXG is divided into two components that are loadable kernel module and library.
Linux-RTXG library is interface of communicate between application and Linux-RTXG core component(kernel module).
it is using system call that is ioctl.

The part of library included speciall method that is independent synchronization method.
it method is used only on the nvidia driver.
if system use nouveau driver, runtime must use part of gdev.
Gdev can happen arbitary interrupt of GPU kernel in the user-space mode, and it have no need to be independent interrupt raised method.

Linux-RTXG loadable kernel module is positioned kernel-space.
Thus, module can use kernel exported function.

\subsection{GPU Scheduling}
\begin{table*}[t]
\begin{center}
\caption{Basic Linux-RTXG APIs}
\label{tab:rtx-api}
\begin{tabular}{|l|p{50em}|} \hline
rtx\_gpu\_open() & To register itself to Linux-RTXG, and create scheduling entity. It will must call first. \\ \hline
rtx\_gpu\_device\_advice() & To get the recommendation of GPU devices to be used \\ \hline
rtx\_gpu\_launch() & To control the GPU kernel launch timing, in other words it is scheduling entry point. It will must call before the CUDA launch API. \\ \hline
rtx\_gpu\_sync() & To wait for finishing GPU kernel execution by sleeping with TASK UNINTERRUPTIBLE status.\\ \hline
rtx\_gpu\_notify() & To send the notify/fence command to GPU microcontroller. The fence or the notify is selected flag is set by argument.\\ \hline
rtx\_gpu\_close() & To release scheduling entity.\\ \hline
\end{tabular}
\end{center}
\end{table*}

Linux-RTXG is API-driven where the scheduler invoked only when computation requests are submitted.
The basic APIs supported by Linux-RTXG are listed in Table~\ref{tab:rtx-api}.
Some APIs have arguments and others do not.
Linux-RTXG APIs are not modification existing CUDA API to cope with proprietary software to be independent from the runtime.
However, user have to add Linux-RTXG api to existing CUDA application for using Linux-RTXG scheduler.

The sample code of the using Linux-RTXG scheduler is shown in Figure~\ref{fig:sample},
and to some extent omitted except GPU scheduling.
GPU tasks may be provided with a function by calling an Linux-RTXG's API at strategic points.

\begin{figure}[t]
\begin{center}
\begin{tabular}{l}
\hline\hline
{\scriptsize \verb|void gpu_task(){        |}\\
{\scriptsize \verb| /* variable initialization  */        |}\\
{\scriptsize \verb| /* calling RESCH API */        |}\\
{\scriptsize \verb|  dev_id = rtx_gpu_device_advice(dev_id); |}\\
{\scriptsize \verb|  cuDeviceGet(&dev, dev\_id);           |}\\
{\scriptsize \verb|  cuCtxCreate(&ctx, SYNC_FLAG, dev);    |}\\
{\scriptsize \verb|  rtx_gpu_open(&handle, vdev_id);     |}\\
{\scriptsize \verb| /* Module load and set kernel function */ |}\\
{\scriptsize \verb| /* Device memory allocation        */ |}\\
{\scriptsize \verb| /* Memory copy to device from host */ |}\\
{\scriptsize \verb|  rtx_gpu_launch(&handle); |}\\
{\scriptsize \verb|  cuLaunchGrid(function, grid_x, grid_y); |}\\
{\scriptsize \verb|  rtx_gpu_notify(&handle); |}\\
{\scriptsize \verb|  rtx_gpu_sync(&handle);   |}\\
{\scriptsize \verb|  /* Memory copy to host from device */  |}\\
{\scriptsize \verb|  /* Release allicated memory */  |}\\
{\scriptsize \verb|}|}\\
\hline\hline
\end{tabular}
\caption{sample code of using rtxg scheduler}
\label{fig:sample}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.5\textwidth]{img/gsched_controlflow.pdf}
\caption{GPU Scheduling control flow}
\end{center}
\label{fig:controlflow}
\end{figure}


Figure~\ref{fig:controlflow} shows control flow of run the Figure\ref{fig:sample}'s sample code.
The configure is the Kernel issue that is restricted to single kernel.
User task (GPU Task) can be control the timing of GPU kernel execution by called $rtx_gpu_launch()$.
Task goes to sleep until the wakeup by interrupt why task is not permitted issuance of GPU kernel due to already execute other task in the GPU.
%The configure is limited to one of  issuance of GPU kerlenel GPUカーネルの発行はひとつに制限されており、すでにGPUでタスクがうごいている状態とし、同期は割込みを用いたNOTIFYによって行うものとする。
%ユーザタスクは$rtx_gpu_launch()$を呼び出すことで、GPUカーネル実行のタイミングをコントロールすることができる。
%既にGPUでタスクが動いており、現在カーネル発行を許可されているタスクが自身でないため、割込みによって起床されるまでtask uninterruptible状態で自らスリープに入る。

Once issued GPU kernel is finished,
interrupt is awaken while the interrupt intercepter wakeup the GPU scheduler, GPU scheduler wakeup the sleeping task. 
The wake-up task issue the GPU kernel via CUDA API such as $cuLaunchGrid()$.
After the GPU kernel issued, task register the NOTIFY for occurring the interrupt,
and task to sleep until it occurs interrupt.
To pick up the next task is performed by the GPU scheduler caused by interruption of GPU kernel finish.
Linux-RTXG is doing the execution order control tasks in the above flow.
%動いていたタスクが終了した時点で割込みが発行され、そのコンテキストのInterrupt intercepterによってスリープしていた次のタスクが起床される。
%起床したタスクは$cuLaunchGrid()$などのCUDA APIを通じてGPUカーネルの発行を行う。
%カーネル発行後、割込みを発生させるためのNotifyの登録を行い、その割込みが発生するまでスリープに入る。
%割込みが発生すると次のタスクが動作するといったフローを持ってスケジューリングが行われる。
%次のタスクの選択は、割込みによって起こされるGPU schedulerによって行われる。

We present hierarchal scheduling which are group scheduling, GPU kernel scheduling.
The group scheduling is a using resource reservation mechanism.
The Context scheduling is a priority scheduling.
Specifically, GPU kernel execution is associated to each scheduling entity while Linux-RTXG grouped the scheduling entity to VGPUs, these VGPUs belong to any of physical GPUs.
In Linux-RTXG, resources are distributed in this group.

Figure~\ref{fig:scheduling} shows pseudo-code of scheduling mechanism.
$on_arrival$ is called when the GPU task is requested GPU kernel launch issue.
In $on_arrival$, GPU task to check whether the given execute permission to group of task itself, and then, check the se permit.
If it has not executing permission, GPU task is enqueued to wait\_queue and go to sleep,
contrary to this, if it has executing permission, GPU task go to launch issue.

$on_completion$ is called by the scheduler thread, when the GPU kernel is completion.
In $on_completion$, scheduler thread pick up the next group, 
%このグループに対して資源の配布 (スケジューリングスレッドによって一定周期で補充)を行い，
%グループ毎に実行許可をスケジューリングポリシー (e.g. BAND, Credit)に従い与えていく.
%GPUタスクは自身のコンテキストが所属するグループに実行許可が与えられているかを確認し実行を行う．
%次タスクの選択はグループ，グループ内のコンテキストと階層的に選択することで，リザベーションに加え優先度スケジューリングとより自由度の高いスケジューリングメカニズムを提供する．
\begin{figure}[t]
\begin{center}
\begin{tabular}{l}
\hline
{\scriptsize \verb| se: The scheduling entity |}\\
{\scriptsize \verb| se->vgpu: The group that is belonged se|}\\
{\scriptsize \verb| se->task: The task that is associated with se |}\\
{\scriptsize \verb| vgpu->parent: The physical GPU identification|}\\
\hline
{\scriptsize \verb|void on_arrival(se) {|}\\
{\scriptsize \verb| check_permit_vgpu(se->vgpu)    |}\\
{\scriptsize \verb| while(!check_permit_se(se)){|}\\
{\scriptsize \verb|   enqueue(se->vgpu,se); |}\\
{\scriptsize \verb|   sleep_task(se->task); |}\\
{\scriptsize \verb| }|}\\
{\scriptsize \verb|}|}\\
{\scriptsize \verb|void on_completion(se) {|}\\
{\scriptsize \verb| reset_the_permit(se->vgpu, se)|}\\
{\scriptsize \verb| n_vgpu = pick_up_the_next_vgpu(se->vgpu->parent) |}\\
{\scriptsize \verb| se = pick_up_the_next_se(n_vgpu)|}\\
{\scriptsize \verb| if(se) {|}\\
{\scriptsize \verb|   dequeue(se->vgpu,se);|}\\
{\scriptsize \verb|   wakeup_task(se->task);|}\\
{\scriptsize \verb| }|}\\
{\scriptsize \verb| set_the_permit(se->vgpu, se)|}\\
{\scriptsize \verb|}|}\\
\hline
\end{tabular}
\caption{High Level Pseudo-code of scheduling mechanisms}
\label{fig:scheduling}
\end{center}
\end{figure}

\subsection{GPU synchronization}
We present implementation of the interrupt intercept is to get the interrupt for kernel free interrupt handling for realized interrupt-driven wakeup the scheduler, and present implementation of independent synchronization mechanism, in Section~\ref{sec:system_model}.
%で説明したruntime environmentに依存しない同期の実現と
%interrupt-drivenなスケジューラ起床を実現するために，kernel freeなまま同期に用いるinterruptを取得するinterrupt interceptを実装する．

Linux-RTXG is use the independent synchronization mechanisms as much as possible,
it because we do not want using black-box resource management for to realized truly real-time resource environments.

%ラウンチされたカーネルが終了したタイミングをスケジューラはNOTIFYかFENCEによって取得する。
%本ワーカースレッドは実行中のカーネルが終了した時点で次のタスクの選択を行う。
%ワーカースレッドはタスクの選択後にCPU資源を他のタスクに明け渡すためにサスペンドに入る。

\textbf{Independent synchronization mechanism from runtime}
We present independent synchronization of NOTIFY and FENCE, these are the to occur interrupt for NOTIFY and the write the fence value by microcontroller.
NVIDIA's proprietary software use ioctl interface to communication between kernel-space and user-space.
These ioctl interfaces are provided drivers function such as device memory allocation, get the GPU information and memory mapping.
Gdev build infrastructure that is able to execute on the NVIDIA's driver using these ioctl interfaces.

We also use this ioctl interface similar to Gdev's command sending method for our method.
Specifically, our methods are two divided, the one is Initialize, the other one is Notify.
%我々はGdevと同様のアプローチでコマンドをSendingする．
%ここでは、ランタイムから独立した割込み機構として、独自にNOTIFY、FENCEに用いるsignを発生させる仕組みを提供する。
%ここでのsignは、NOTIFYは割込み、FENCEはmappedメモリへの値の書き込みである。
%NVIDIAのクローズドソースドライバはNouveauプロジェクトのリバースエンジニアリングによる解析によって、ioctlを使ったインタフェースになっていることがわかっている。
%Gdevではこの解析された情報を用いて、NVIDIAのクローズドソースドライバとオープンソースライブラリという掛け合わせでCUDAを実行できる基盤が構築されている。
%This method 
%本メソッドは大きく2つに分かれ、それぞれInitializeとNotifyと呼ぶ。
Initialize processes for generating a context dedicated this method.
These processes are including the create virtual address space and the allocate indirect buffer object for command sending and the create context object.
The indirect buffer is an area for storing GPU commands

The Notify processes send commands to the compute engine or the copy engine that are
%Initializeは、いわゆるコンテキストの生成に値する。Virtual Address Spaceやコマンド送信に用いるIndirect Bufferの確保、コンテキストオブジェクトの生成などを行う。
%NotifyはComputeエンジンやCopyエンジンに向けて割込み発生、もしくはFENCE用に値の書き込みを行うコマンドを送信する。
This independent generate sign for synchronization method is using reverse engineering.
However, It is the limit of implementation using the closed-source runtime environment.

%本アプローチに用いるインタフェースは公式にサポートされていないために、ベンダーによる急な仕様変更には対応できない。
%しかしながら、これ以外に割込みを発生させるアプローチがなく、クローズドソースを用いた場合の限界であるといえる。

\textbf{Interrupt interception:}
Interrupts are handled by the ISR (Interrupt Service Routine) that is registered kernel by the device driver.
In addition, scheduler require to identify the interrupt by using readling GPU status register, it must be done before original ISR is reset the GPU status register.

%割込みはデバイスドライバ（カーネルと共にパッケージされている）によって登録されたISRがハンドルする。
%加えて、割込みの識別はGPUのステータス・レジスタを読み込んで行う必要があり、
%GPUドライバが割込みレジスタをリセットする前に、実行される必要がある。
Linux kernel have structure that holds the interrupt parameters called irq\_desc for each interrupt number.
These structures have structures called irq\_action including the ISR callback pointer.
irq\_desc is allocated to global memory space of the kernel, anyone is accessible from kernel space.
Linux loadable kernel modules can get an irq\_desc for running in kernel, while also can get an callback pointer of ISR.
We retain getting callback pointer of GPU device driver's ISR, and then we register interrupt interception ISRs to kernel.
So, we get the to intercept interrupt by it ISR and then call retaining callback pointer, 
%Linuxでは、割込み番号ごとにirq\_descという割込みのパラメータを保持する構造体を持っている。
%この構造体にはISRの関数ポインタを含むirq\_actionという構造体がリストで接続されている。
%irq\_descはグローバルな領域に確保されており、カーネル空間からであれば誰でも参照可能である。
%Linuxのローダブルカーネルモジュールはカーネル空間で動作しているため、このirq\_descを取得でき、
%Interrupt handlerの関数ポインタも取得可能である。
%我々はこの取得した関数ポインタを保持し、我々の傍受用ISRをカーネルに登録する。
%そして傍受用ISRで、事前に保持しておいたGPUドライバの割込みハンドラの関数コールバック関数として呼び出すことで、通常の割込みハンドリングを実行する。
In addition, I/O registers are mapped to kernel memory space by device driver from the PCIe base address registers (BARs)\cite{fujii:icpads2013,kato2013zero}.
Therefore Linux-RTXG remap the BAR0 to our allocated space by using $ioremap()$ when the ISR initizalize.
The interrupt interception identify interrupt by read the mapped-space.

%加えて我々のこれまでの研究\cite{fujii:icpads2013,kato2013zero}で、GPUのio registerはPCIeのBAR0によって指定されたアドレスから存在しておりカーネル空間にデバイスドライバによってマッピングされていることがわかっている。
%そのためLinux-RTXGが傍受用ISRの初期化の際に$ioremap()$によってBAR０空間をマッピングしておき、傍受用ISRが呼び出された際にマッピングされたレジスタを読み込むことで、
%割込みの識別を行う。
\subsection{Scheduler Integration}
Linux scheduler have various real-time scheduling policies that were SCHED\_DEADLINE, SCHED\_FIFO and SCHED\_RR.
, SCHED\_DEADLINE is implementation the Constant Bandwidth Server and Global Earliest Deadline First,
while it is including mainline of Linux 3.14.0 kernel.
%特にSCHED\_DEADLINEはLinux 3.14よりメインラインに含まれたConstant Bandwidth ServerとGlobal-EDFの実装であり、
%Linuxをリアルタイム拡張するにおいて有効に利用できるクラスである。
However, synchronization does not work well in a SCHED\_DEADLINE scheduling policy when using GPU tasks.

This problems are twofold.
The first is implementation of sched\_yield---in kernel space used yield()---.
The second is implementation of return from sleeping state.

%本問題は2種類存在しており、
%sched\_yield()によるCPU放棄の実装によるものと、%suspendingした後の復帰の実装によるものである。

The first problem occurs by releasing the CPU using sched\_yield() when waiting for I/O in polling.
Polling (Spin) is the exclusive CPU, therefore task may once better to release the CPU can obtain good results.
However, sched\_yield will set 0 to polling task's runtime of remaining execution time treated as a parameter of SCHED\_DEADLINE.
%一つ目のsched\_yield(カーネル内ではyield関数)は、FENCEのようにPollingされる場合に生じる。
%pollingはCPUを専有してしまう方式であり、他のタスクへの影響を考えた場合、一度CPUを放棄したほうが良い結果が得られる場合がある。
%しかしながらsched\_yieldでは、SCHED\_DEADLINEの内部パラメータとして扱う、runtime（残りの実行しても良い時間）を0にしてしまう。
Thereby, it task lose execute authority until runtime is replenished in the next period, therefore task is unable to call sched\_yield between polling.
%これによって、次の周期が訪れるまではruntimeが補充されることがなく、そのタスクは実行権限を失う。
sched\_yield is used much by device drivers and library as well as GPU runtime.
These software is affected by this problem.
Even NVIDIA CUDA is affected depending on the setting.
We support this problem by limit the GPU synchronization method to NOTIFY  in the SCHED\_DEADLINE policies.
%Linux-RTXGではSCHED\_DEADLINE時はNOTIFYを使うことを推奨し、sched\_yieldの利用を制限することで対応した。
%sched\_yieldはGPUランタイムに限らず、デバイスドライバやライブラリなどで多く利用されており、
%それら全てで、SCHED\_DEADLINEポリシー上では正常に動作しない結果が生じる可能性が高い。
%NVIDIAのCUDAにおいても同期フラグの設定次第で本問題に影響を受ける。

The second problem is subjected to a check equation (1) when restore task from sleeping state.
If equation (1) is true, runtime is replenished and absolute deadline is setted next cycle deadline.
%２つ目の問題はタスクが一度sleeping状態に入り、復帰時に式(1)を用いて実行可能性についてチェックを行う。
%式(1)が真の時、runtimeが補充され、absolute deadlineが次の周期に設定される。

{\scriptsize
\begin{equation}
\frac{Absolute\_Deadline - Current\_Time}{Remaining\_Runtime} > \frac{Relative\_Deadline}{Period}
\end{equation}
}

We corresponding to this check by
subtracting the GPU execution time from $Remaining\_Runtime$
when task is restored by GPU kernel execution with the exception of the task is restored by period.
%Linux-RTXGでは本チェックについて、sleeping状態から起床するという状態を、
%GPUカーネル実行による復帰と、周期による復帰とで種類分けし、
%GPUカーネル実行による復帰時にのみ、$Remaining\_Runtime$からGPU execution timeを引くことで対応した。

