\SECTION{Introduction}
Graphics processing units (GPUs) are now common platforms for various
data-parallel and compute-intensive applications.
Although GPUs are primarily used to accelerate high-performance
computing (HPC) applications, their performance advantage is more and
more recognized in real-time systems.
Examples include route navigation for autonomous
vehicles~\cite{cmu:routing}, object
detection~\cite{hirabayashi:cpsna2013}, plasma control for fusion 
reactors~\cite{tokamak}, windowing applications~\cite{kato:rtas2011},
and database operators~\cite{bakkum:sql}.
Benchmarking suites are also well deployed for various pieces of
workload~\cite{rodinia}.

GPU applications were often best-effort oriented, whose main purpose is
to accelerate particular computing blocks.
Therefore, conventional GPU technologies are particularly designed for
individual data-parallel and compute-intensive workload.
However, due to emergence of real-time systems using GPUs, system
software support for real-time GPU resource management is becoming a
more significant problem.
Since the standard system software released by GPU vendors and
communities, such as CUDA~\cite{nvidia:cuda_zone} and
OpenCL~\cite{opencl}, is not tailored to support real-time systems, a
complex undertaking is required to extend the system software including
the operating system (OS) kernel and device drivers.

In previous work, GPU resource management has been developed for
real-time systems.
TimeGraph~\cite{kato:timegraph} provides GPU scheduling and resource
reservation capabilities at the device driver level, multiplexing GPU
commands to support priorities and quality of service (QoS) for GPU
applications.
Gdev~\cite{kato:gdev} is a rich set of runtime libraries and device
drivers to achieve first-class GPU resource management, where GPU
contexts are fully managed in the OS space.
The main drawback of these work is that they require detailed
information on the implementation of GPU runtime libraries 
and device drivers, mostly obtained based on reverse engineering.

In general, the GPU software stack is encapsulated by an application
programming interfance (API) in runtime libraries and an application
binary interface (ABI) in device drivers.
This means that modifications to system software on top of the API and
ABI layers allow the entire software stack to be loadable, enabling more
sustainable solutions.
CPU scheduling is also important for GPU applications, because CPU time
is consumed when GPU functions are launched from the API and ABI layers.
In order to satisfy real-time requirements in GPU computing, therefore,
CPU and GPU resources must be managed in coordination.

One of notable work on coordinated CPU and GPU resource management for
real-time systems is GPUSync~\cite{elliott:gpusync13,elliott:explor14},
which was presented by Elliott et al. to extend CPU scheduling for
multiple GPU-aware contexts with budget enforcement.
GPUSync was built on top of on the API and ABI layers of proprietary
software, providing a configurable framework that can verify the
combination of task allocation policies for multicore CPUs and GPU
resource allocaton policies for multiple GPUs.

GPUSync is implemented using $LITMUS^{RT}$~\cite{litmus}, which
introduces a significant amount of changes to the OS kernel.
TimeGraph and Gdev also make some modifications to the device driver.
Those built-in approaches to the OS kernel and device drivers require
users to install patches, or developers are obliged to maintain patches
in order to stay up to date with the latest version releases.
This porting work is a complex undertaking against research and
development and is not sustainable, given that especially open-source
software, such as Linux, is frequently updated with non-trivial code
changes.

Linux supports the loadable kernel module (LKM), which can load and
unload kernel modules to add and remove supplementary kernel functions.
Existing Linux-based real-time OSes use the LKM to extend real-time
capabilities of Linux.
In particular, RESCH~\cite{kato2009loadable, asberg2012exsched} provides
a real-time scheduling framework called ExSched, which is completely
independent of code modifications to the OS kernel and device drivers.
Unfortunately, RESCH does not support GPU resource management.
It has never been demonstrated that GPU resource management for
real-time systems can be fully implemented by the LKM, without making
any modification to the OS kernel and device drivers.

\textbf{Contribution:}
This paper presents Linux-RTXG (Linux Real-Time eXtention with GPUs),
which provides LKM-based real-time GPU resource management in Linux.
Linux-RTXG allows the system to easily re-configure scheduling
algorithms and install their modules at runtime for GPU applications.
The most significant contribution of Linux-RTXG is that resource
management modules for not only CPUs but also GPUs can be added to Linux
without any modification to the OS kernel and device drivers.
CPU scheduling and resource reservation mechanisms are based on RESCH,
while GPU scheduling and resource reservation mechanisms are implemented
using Gdev.
In addition to the integration of RESCH and Gdev, Linux-RTXG provides a
new framework to coordinate CPU and GPU resource management, freeing
from the built-in OS kernel and device drivers, thus is a competely
patch-free approach.

\textbf{Organization}
The rest of this paper is organized as follows.
Section~\ref{sec:system_model} describes the system model and basic
approaches for Linux-RTXG.
Section~\ref{sec:design_imple} presents the design and implementation of
Linux-RTXG with a particular emphasis on GPU scheduling.
Section~\ref{sec:evaluation} evaluates the system overhead and
reservation performance of Linux-RTXG.
Section~\ref{sec:relatedwork} discusses related work, and this paper is
concluded in Section~\ref{sec:conclusion}.
