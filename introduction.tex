\SECTION{Introduction}
Graphics processing units (GPUs) are now common platforms for various data-parallel and compute-intensive applications. Although GPUs are primarily used to accelerate high-performance-computing (HPC) applications, their performance advantage is increasingly recognized in regard to real-time systems. Examples of such GPU applications include route navigation for autonomous vehicles~\cite{cmu:routing}, object detection~\cite{hirabayashi:cpsna2013}, plasma control for fusion reactors~\cite{tokamak}, windowing applications~\cite{kato:rtas2011}, and database operators~\cite{bakkum:sql}. Benchmarking suites for various workloads~\cite{rodinia} are also well deployed. 

GPU applications, whose main purpose is to accelerate particular computing blocks, were often best-effort oriented. Conventional GPU technologies are therefore particularly designed for individual data-parallel and compute-intensive workloads. However, due to an emergence of real-time systems using GPUs, supporting system software for real-time management of GPU resources is becoming a more significant problem. Since the standard system software released by GPU vendors and communities, such as CUDA~\cite{nvidia:cuda_zone} and OpenCL~\cite{opencl}, is not tailored to support real-time systems, extending the system software (including the operating-system (OS) kernel and device drivers) requires a complex undertaking.

In previous work, GPU resource management for real-time systems was developed. In detail, TimeGraph~\cite{kato:timegraph} provides GPU scheduling and resource reservation capabilities at the device-driver level, multiplexing GPU commands to support priorities, and quality of service (QoS) for GPU applications. Gdev~\cite{kato:gdev} is a rich set of runtime libraries and device drivers that achieves first-class GPU resource management, where GPU contexts are fully managed in the OS space. The main drawback of these work is that they require detailed information on the implementation of GPU runtime libraries and device drivers, mostly obtained by reverse engineering.

In general, a GPU software stack is encapsulated by an application-programming interface (API) in runtime libraries and an application binary interface (ABI) in device drivers. This means that modifications to system software outside of the API and ABI layers allow the entire software stack to be loadable, enabling more sustainable solutions. CPU scheduling is also important for GPU applications, because CPU time is consumed when GPU functions are launched from the API and ABI layers. To satisfy real-time requirements in regard to GPU computing, therefore, CPU and GPU resources must be managed in coordination.

Presented by Elliott et al., one notable work on coordinated CPU and GPU resource management for real-time systems is GPUSync~\cite{elliott:gpusync13,elliott:explor14}, which was intended to extend CPU scheduling for multiple GPU-aware contexts with budget enforcement. GPUSync was built on top of the API and ABI layers of proprietary software, providing a configurable framework that can verify the combination of task-allocation policies for multicore CPUs and GPU-resource-allocation policies for multiple GPUs.

GPUSync is implemented by using $LITMUS^{RT}$~\cite{litmus}, which introduces a significant amount of changes to the OS kernel. TimeGraph and Gdev also make some modifications to the device driver. Those built-in approaches to the OS kernel and device drivers require users to install patches or developers to maintain patches in order to stay up to date with the latest version releases. This so-called ``porting'' work is a complex undertaking against research and development and is not sustainable, given that open-source software, such as Linux, especially, is frequently updated with non-trivial code changes.

Linux supports the loadable kernel module (LKM), which can load and unload kernel modules so as to, respectively, add and remove supplementary kernel functions. Existing Linux-based real-time OSs use an LKM to extend real-time capabilities of Linux. In particular, RESCH~\cite{kato2009loadable, asberg2012exsched} provides a real-time scheduling framework, called ExSched, which is completely independent of code modifications to the OS kernel and device drivers. Unfortunately, RESCH does not support GPU resource management. It has never been demonstrated that GPU resource management for real-time systems can be fully implemented by using an LKM, without making any modifications to the OS kernel and device drivers.

\textbf{Contribution:} An LKM framework called ``Linux-RTXG (Linux Real-Time eXtention with GPUs),'' which provides LKM-based real-time GPU resource management in Linux, is proposed. Linux-RTXG allows the system to easily re-configure scheduling algorithms and install their modules at runtime for GPU applications. The most significant contribution of Linux-RTXG is that resource management modules for not only CPUs but also GPUs can be added to Linux without any modification to the OS kernel and device drivers. CPU mechanisms for scheduling and resource reservation are based on RESCH, while GPU mechanisms for scheduling and resource reservation are implemented by using Gdev. In addition to integrating RESCH and Gdev, Linux-RTXG provides a new framework to coordinate CPU and GPU-resource management, freeing them from the built-in OS kernel and device drivers. It is thus a completely ``patchless'' approach.

\textbf{Organization} The rest of this paper is organized as follows. Section~\ref{sec:system_model} describes the system model and basic approaches for Linux-RTXG. With a particular emphasis on GPU scheduling, Section~\ref{sec:design_imple} presents the design and implementation of Linux-RTXG. Section~\ref{sec:evaluation} presents the results of an evaluation of the system overhead and reservation performance of Linux-RTXG. Section~\ref{sec:relatedwork} discusses related work, and Section~\ref{sec:conclusion} concludes the paper. 

