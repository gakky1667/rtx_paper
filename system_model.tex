\SECTION{System Model}
\label{sec:system_model}
A model of GPU programming and GPU scheduling is assumed and described as follows.
To highlight an unsolved problem concerning GPU resource management, existing work is introduced first.
In addition to that problem, factors that have prevented GPU resource management from patch-free implementation are described.
As for the model, the system is assumed to be composed of multiple GPUs and multi-core CPUs.

\SUBSECTION{GPU Programming Model}
General-purpose computing on GPUs (GPGPU) is supported by special programming languages such as CUDA and OpenCL.
In this work, it is assumed that GPU application programs are written in CUDA; however, the concept of GPU resource management studied in this work is not limited by programming languages.
Conceptually, the contribution of this work is applicable to OpenCL and other languages.
A GPU task is defined as a process running on the CPU that launches a GPU kernel to the GPU, whose cyclic execution unit is referred to as a ``GPU job.''
The GPU kernel is the process that is executed on the GPU.
Multi-tasking environments, in which multiple tasks of GPU applications are allowed to coexist, are also assumed, though the current model of GPU programming does not allow multiple contexts to be executed at the same time on the GPU.
In other words, multiple GPU contexts can be created and launched, but they must be executed exclusively on the GPU.

GPU programming requires a set of APIs, such as CUDA Driver API and CUDA Runtime API, provided by runtime libraries.
A typical approach to GPU programming follows several steps: (i) \textit{cuCtxCreate} creates a GPU context; (ii) \textit{cuMemAlloc} allocates memory space to the device memory; (iii) \textit{cuModuleLoad} and \textit{cuMemcpyHtoD} copy the data and the GPU kernel from the host memory to the memory space of the allocated device; (iv) \textit{cuLaunchGrid} invokes the GPU kernel; (v) \textit{cuCtxSynchronize} synchronizes a GPU task to wait for the completion of GPU kernel; (vi) \textit{cuMemcpyDtoH} transfers the data back to the host memory from the device memory; and (vii) \textit{cuMemFree}, \textit{cuModuleUnload}, and \textit{cuCtxDestroy} release the allocated memory space and the GPU context.

\SUBSECTION{GPU Scheduling}
Problems concerning resource management in many variants of real-time OSes (RTOSes)~\cite{spring,redline,itron,rk} have been addressed.
Of particular interest includes those with Linux-based RTOSes~\cite{litmus,prk,rtai,yodaiken1999rtlinux,kato2009loadable}.
GPUs are also supported in Linux.
It is assumed in this work that the OS architecture refers to Linux.

To meet real-time requirements in multi-tasking environments, RTOSes should provide \textit{scheduling} and \textit{resource reservation} capabilities.
Rate Monotonic (RM) and Earliest Deadline First (EDF)~\cite{sched:ll}) are well-known algorithms for priority-driven scheduling for real-time systems.
Resource-reservation algorithms come in many variants, such as Constant Bandwidth Server (CBS)~\cite{rr:cbs} and Total Bandwidth Server (TBS)~\cite{rr:tbs2}.

GPU computing must deal with the data-transfer bandwidth and compute cores as shared resources.
In case of real-time systems, therefore, scheduling and resource reservation for the GPU as well as the CPU must be considered.
TimeGraph and Gdev are involved in solving problems concerning GPU resource management, but they are not much aware of the fact that GPU tasks consume CPU time to drive APIs.
To support GPUs in real-time systems, the OS scheduler must be able to manage GPU tasks in coordination with CPU tasks on the host side, while monitoring GPU time for GPU kernels.
Therefore, the problem of CPU and GPU coordinated scheduling is considered in term of resource-reservation mechanisms.

The recently developed GPUSync framework indeed employs CPU and GPU coordinated scheduling.
In GPUSync, the device driver and runtime library for GPU computing are hidden in black-box modules released by GPU vendors.
On the other hand, TimeGraph and Gdev address this problem by using reverse-engineered open-source software.
Both approaches are limited to some extent.
That is to say, using black-box modules makes it difficult to manage the system in a fine-grained manner.
Open-source software tends to lack some functionality due to incomplete reverse engineering.
GPUSync manages to incorporate black-box modules in scheduling and resource reservation by arbitrating interrupt handlers and runtime accesses.
This GPUSync approach is preferred in the sense a reliable proprietary driver and library can be utilized, while functions for scheduling and resource reservation can still be provided.

\textbf{GPU Synchronization:}
 Given that the GPU is a coprocessor connected to the host CPU, synchronization between the GPU and the CPU must be considered to guarantee the correctness of execution logics.
Most GPU architectures support two synchronization mechanisms.
One is based on memory-mapped registers called ``FENCE.''
To use FENCE, it is necessary to send special commands to the GPU when a GPU kernel is launched so that the GPU can write the specified value to this memory-mapped space when the GPU kernel is completed.
On the host side, a GPU task is polling this value via the mapped space.
The other technique is interrupt-based synchronization called ``NOTIFY.''
In a similar manner to FENCE, to use NOTIFY, it is also necessary to send special commands to the GPU.
Instead of writing to memory-mapped registers, NOTIFY raises an interrupt from the GPU to the CPU and, at the same time, writes some associated values to I/O registers of the GPU.
On the host side, a GPU task is suspended so that it waits for the occurrence of the interrupt.
NOTIFY allows other tasks to use CPU resources when the GPU task is waiting for completion of execution of its GPU kernel, though a scheduling overhead is involved.
FENCE is easier to use and is more responsive than NOTIFY, but it is implemented at the expense of CPU utilization.
More details about GPU architectures can be found in a previous work~\cite{kato:timegraph, kato:gdev, fujii:apsys2013}.

To synchronize the CPU and the GPU, Gdev supports both NOTIFY and FENCE.
NOTIFY is primarily used for scheduling of GPU tasks, while FENCE is used in driver-level synchronization.
In Gdev, as aforementioned, the implementation of GPU synchronization involves additional commands to the GPU.
The device driver must thus be modified to a certain extent.
GPUSync indirectly utilizes the NOTIFY technique with tasklet interception~\cite{elliott2012robust} on top of the proprietary black-box modules.
``Tasklet'' refers to Linux's softirq implementation.
GPUSync identifies the interrupt that has invoked a GPU kernel using a callback pointer with a tasklet.

\textbf{Loadable Kernel Modules:}
 The main concept of our system is to enable both the CPU and GPU to be managed by the OS scheduler without any code changes to the OS kernel and device drivers.
CPU scheduling under this constraint has already been demonstrated by RESCH~\cite{kato2009loadable, asberg2012exsched}.
To schedule GPU tasks, it must be possible to hook the scheduling points where the preceding GPU kernel is completed and the active context is switched to the next GPU kernel.
The scheduling points can be hooked by two methods.
The API-driven method (presented by RGEM~\cite{kato:rgem}) explicitly awakens the scheduler after GPU synchronization, such as $cuCtxSynchronize()$, invoked by the API.
The interrupt-driven method (presented by TimeGraph and Gdev), on the other hand, uses interrupts that can be configured by NOTIFY.
GPUSync is also based on this interrupt-driven method.
Especially in the case of CUDA, the standard $cuCtxSynchronize()$ API synchronizes completion of all GPU kernels.
Therefore, the API-driven method can be used if a GPU context issues no more than one GPU kernel.
In other words, if a GPU task invokes multiple GPU kernels, the interrupt-driven method is more appropriate to realize real-time capabilities.


The interrupt-driven method forces TimeGraph, Gdev, and GPUSync to modify the code of either the Linux kernel or device drivers.
This modification is necessary because the interrupt service routine (ISR) must be managed to create the scheduling points.
Gdev has developed independent synchronization mechanisms on top of the proprietary software; however, for scheduling and resource reservation on Gdev, the Linux kernel still needs some modification.
As a result, the available release versions of the Linux kernel and device drivers for TimeGraph, Gdev, and GPUSync are very limited.
A core challenge of this work is to develop independent synchronization mechanisms that do not require any modification to the OS kernel and device drivers so that a coordinated CPU and GPU resource-management scheme can be utilized with a wide range of the Linux kernel and device drivers.

\textbf{Scope and Limitation:} 
GPU resource management has a ``non-preemptive'' nature.
For example, the execution of GPU kernels is non-preemptive.
The data transfer between the CPU and the GPU is also non-preemptive.
Previous works have addressed the problem of response time by preventing overrun that occurs while the kernel is being divided~\cite{basaran:preemptive,sparc}.
The most difficult problem is scheduling of self-suspending tasks because the GPU is treated as an I/O device in the system.
For example, GPU tasks are suspended until their GPU kernels are completed.
This ``self-suspending'' problem was presented as a NP-HARD problem in previous works~\cite{self-sus:1,self-sus:2}.
A lot of work on the scheduling of self-suspending tasks, however, is ongoing~\cite{chattopadhyay2014limited,kim2013segment}.

Such a ``schedulability'' problem of GPU scheduling out of the scope of this work.
Instead, the design and implementation of GPU scheduling and resource reservation with existing algorithms is focused on.
The scope of this work is also restricted to time resources (but not to memory resources).
In other words, GPU-scheduling problems, not device-memory-allocation problems, are considered in the following.

The proposed system supports efficient data transfer between the CPU and the GPU by using GPU microcontrollers~\cite{fujii:icpads2013}, but that support is not considered as a contribution of this work.
Finally, our prototype system is limited to a Linux system and CUDA environment; however, the concept of our method is applicable to other OSes and programming languages, as far as they support the aforementioned FENCE and NOTIFY primitives.
