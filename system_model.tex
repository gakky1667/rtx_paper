\SECTION{System Model}\label{sec:system_model}
This section describes the model of GPU programming and GPU scheduling
assumed in this paper.
We also introduce existing work to highlight an unsolved problem of GPU
resource management.
In addition to the problem of GPU resource management, we argue what has
prevented GPU resource management from patch-free implementation. 
The system is assumed to be composed of multiple GPUs and multi-core
CPUs.

\SUBSECTION{GPU Programming Model}
General-purpose computing on GPUs (GPGPU) is supported by special
programming languages, such as CUDA and OpenCL.
This work assumes that GPU application programs are written in CUDA;
however, the concept of GPU resource management studied in this paper is
not limited by programming languages.
Conceptually, the contribution of this work is applidable to OpenCL and
other languages.
We define a GPU task as a process running on the CPU that launches a GPU
kernel to the GPU, whose cyclic execution unit is referred to as a GPU
job.
The GPU kernel is a process that is executed on the GPU. 
We also assume multi-tasking environments where multiple tasks of GPU
applications are allowed to coexisit, though the current model of GPU
programming does not allow multiple contexts to execute at the same time
on the GPU.
In other words, we can create and launch multiple GPU contexts, but they
must be executed exclusively on the GPU.

GPU programming requires a set of APIs provided by runtime libraries,
such as CUDA Driver API and CUDA Runtime API.
A typical approach to GPU programming follows several steps:
(i) \textit{cuCtxCreate} creates a GPU context;
(ii) \textit{cuMemAlloc} allocates memory space to the device memory; 
(iii) \textit{cuModuleLoad} and \textit{cuMemcpyHtoD} copy the data and
the GPU kernel from the host memory to the allocated device memory
space;
(iv) \textit{cuLaunchGrid} invokes the GPU kernel;
(v) \textit{cuCtxSynchronize} synchronizes a GPU task to wait for the
completion of GPU kernel;
(vi) \textit{cuMemcpyDtoH} transfers the data back to the host memory
from the device memory; and
(vii) \textit{cuMemFree}, \textit{cuModuleUnload}, and \textit{cuCtxDestroy} release the allocated memory space and the GPU context.

\SUBSECTION{GPU Scheduling}
Resource management problems have been addressed in many variants of
real-time OSes (RTOSes)~\cite{spring,redline,itron,rk}.
Of particular interest includes those in Linux-based
RTOSes~\cite{litmus,prk,rtai,yodaiken1999rtlinux,kato2009loadable}.
GPUs are also supported in Linux.
This work assumes that the OS architecture refers to Linux.

To meet real-time requirements in multi-tasking environments, RTOSes
should provide \textit{scheduling} and \textit{resource reservation}
capabilities.
Rate Monotonic (RM) and Earliest Deadline First (EDF)~\cite{sched:ll})
are well-known algorithms of priority-driven scheduling for real-time
systems.
There are also many variants of resource reservation algorithms.
Examples include Constant Bandwidth Server (CBS)~\cite{rr:cbs} and Total
Bandwidth Server (TBS)~\cite{rr:tbs2}.

GPU computing must deal with the data transfer bandwidth and compute
cores as shared resources.
Therefore, scheduling and resource reservation must be considered for
the GPU as well as the CPU in real-time systems.
TimeGraph and Gdev are involved with GPU resource management problems
but are not much aware of the fact that GPU tasks consume CPU time to
drive APIs.
To support GPUs in real-time systems, the OS scheduler must be able to
manage GPU tasks in coordination with CPU tasks on the host side, while
monitoring GPU time for GPU kernels.
Therefore, we consider the problem of CPU and GPU coordinated scheduling
with resource reservation mechanisms.

The recently developed GPUSync framework indeed employs CPU and GPU
coordinated scheduling.
In GPUSync, the device driver and runtime library for GPU computing are
hidden in black-box modules released by GPU vendors.
On the other hand, TimeGraph and Gdev address this problem by using
reverse-engineered open-source software.
Both approaches are limited to some extent, respectively.
Using black-box modules makes it difficult to manage the system in a
fine-grained manner.
Open-source software tends to lack some functionality due to incompleted
reverse engineering.
GPUSync managed to incorporate black-box modules in scheduling and
resource reservation by arbitrating interrupt handlers and runtime
accesses.
This GPUSync approach is preferred in a sense that we can utilize
reliable proprietary driver and library, while we can still provide
scheduling and resource reservation functions.

\textbf{GPU Synchronization:}
Given that the GPU is a coprocessor connected to the host CPU,
synchronization between the GPU and the CPU must be considered to
guarantee the correctness of execution logics.
Most of GPU architectures support two synchronization mechanisms.
One is based on memory-mapped registers called FENCE.
To use FENCE, we send special commands to the GPU when a GPU kernel is
launched so that the GPU can write the specified value to this
memory-mapped space when the GPU kernel is completed. 
On the host side, a GPU task is polling to monitor this value via the
mapped space.
The other technique is interrupt-based synchronization called NOTIFY.
To use NOTIFY, we also send special commands to the GPU similar to
FENCE.
Instead of writing to memory-mapped registers, NOTIFY raises the
interrupt from the GPU to the CPU and at the same time writes some
associated values to I/O registers of the GPU.
On the host side, a GPU task is suspended to wait for the occurrence of
interrupt.
NOTIFY allows other tasks to use CPU resources when the GPU task is
waiting for completion of its GPU kernel, though the scheduling overhead
is involved.
FENCE is easier to use and is more responsive than NOTIFY but is
implemented at the expense of CPU utilization.
More details about GPU architectures can be found in previous
work~\cite{kato:timegraph, kato:gdev, fujii:apsys2013}.

Gdev supports both NOTIFY and FENCE to synchronize the CPU and the GPU.
NOTIFY is primarily used for scheduling of GPU tasks, while FENCE is
used in driver-level synchronization. 
In Gdev, the implementation of GPU synchronization involves additional
commands to the GPU as aforementioned, which requires some modification
to the device driver.
GPUSync indirectly utilizes the NOTIFY technique with tasklet
interception~\cite{elliott2012robust} on top of the proprietary
black-box modules.
Tasklet refers to Linux's soft-irq implementation.
GPUSync identifies the interrupt that has invoked a GPU kernel using a
callback pointer with a tasklet.

\textbf{Loadable Kernel Modules:}
The main concept of our system is to enable both the CPU and GPU to be
managed by the OS scheduler without any code changes to the OS kernel
and device drivers.
CPU scheduling has already been demonstrated by
RESCH~\cite{kato2009loadable, asberg2012exsched} under this constraint.
In order to schedule GPU tasks, we must be able to hook the scheduling
points where the preceding GPU kernel is completed and the active
context is switched to the next GPU kernel.
The scheduling points can be hooked by two methods.
The API-driven method presented by RGEM~\cite{kato:rgem} explicitly
awakens the scheduler after GPU synchronization invoked by the API, such
as $cuCtxSynchronize()$.
The interrupt-driven method presented by TimeGraph and Gdev, on the
other hand, uses interrupts that can be configured by NOTIFY.
GPUSync is also based on this interrupt-driven method.
Especially in CUDA, the standard $cuCtxSynchronize()$ API synchronizes
completion of all GPU kernels.
Therefore, the API-driven method can be used if a GPU context issues no
more than one GPU kernel.
In other words, if a GPU task invokes multiple GPU kernels, the
interrupt-driven method is more approapriate to realize real-time
capabilities.

The interrupt-driven method forced TimeGraph, Gdev, and GPUSync to
modify the code of either the Linux kernel or device drivers.
This is because the interrupt service routine (ISR) must be managed to
create the scheduling points.
Gdev has developed independent synchronization mechanisms on top of the
proprietary software; however, Gdev still needs some modification to the
Linux kernel for scheduling and resource reservation.
As a result, the available release versions of the Linux kernel and
device drivers for TimeGraph, Gdev, and GPUSync are very limited.
A core challenge of this paper is to develop independent synchronization
mechanisms that do not require any modification to the OS kernel and
device drivers so that we can utilize the coordinated CPU and GPU
resource management scheme with a wide range of the Linux kernel and
device drivers. 

\textbf{Scope and Limitation:}
GPU resource management is involved with non-preemptive nature.
For example, the execution of GPU kernels is non-preemptive.
The data transfer between the CPU and the GPU is also non-preemptive.
Some previous work have addressed the problem of response time by
preventing overrun that occurs while dividing the
kernel~\cite{basaran:preemptive,sparc}.
The most difficult problem is the scheduling of self-suspending tasks
because the GPU is treated as an I/O device in the system. 
For example, GPU tasks are suspending until their GPU kernels are
completed.
The self-suspending problem was presented as a NP-HARD problem in
previous work~\cite{self-sus:1,self-sus:2}.
There are a lot of ongoing work on the scheduling of self-suspending
tasks~\cite{chattopadhyay2014limited,kim2013segment}.

Such a schedulability problem of GPU scheduling is not the scope of this
paper.
We rather focus on the design and implementation of GPU scheduling and
resource reservation with existing algorithms.
The scope of this paper is also restricted to time resources but not
memory resources.
In other words, we consider GPU scheduling problems but not device
memory allocation problems.

We support efficient data transfer between the CPU and the GPU using
GPU microcontrollers~\cite{fujii:icpads2013}, but is not considered as
the contribution of this paper.
Finally, our prototype system is limited to the Linux system and the
CUDA environment, but the concept of our method is applicable to other
OSes and programming languages, as far as they support the
aforementioned FENCE and NOTIFY primitives.
