\SECTION{System Model}\label{sec:system_model}
This section describes the model of GPU programming and GPU scheduling
assumed in this paper.
We also introduce some existing work to highlight a unsolved problem of
GPU resource management.
In addition to the problem of GPU resource management, we argue the
obstacle that has prevented GPU resource management from patch-free
implementation. 
Note that this work focuses on such systems that are composed of
multiple GPUs and multi-core CPUs.

\SUBSECTION{GPU Programming Model}
General-purpose computing on GPUs (GPGPU) is supported by special
programming languages, such as CUDA and OpenCL.
This work assumes that GPU application programs are written in CUDA;
however, the concept of GPU resource management studied in this paper is
not limitted by programming languages, i.e., the contribution of this
work is applidable to OpenCL.
We define a GPU task as a process running on the CPU that executes a
function to launch a GPU kernel to the GPU, whose cyclic execution unit
is referred to as a GPU job.
The GPU kernel is a process that is executed on the GPU. 
Note that we also assume multi-tasking environments where multiple tasks
of GPU applications are allowed to coexisit, though the current model of
GPU programming does not allow multiple contexts to execute at the same
time on the GPU but kernels from the same context.
In other words, we can create and launch multiple GPU contexts, but they
must be executed exclusively on the GPU by hardware nature.

GPU programming requires a set of APIs provided by runtime libraries
such as CUDA Driver API or CUDA Runtime API.
Typically, GPU application programs are executed through the following
steps:
(i) \textit{cuCtxCreate} creates a GPU context;
(ii) \textit{cuMemAlloc} allocates memory space to the device memory; 
(iii) \textit{cuModuleLoad} and \textit{cuMemcpyHtoD} copy the data and
the GPU kernel from the host memory to the allocated device memory
space;
(iv) \textit{cuLaunchGrid} invokes the GPU kernel;
(v) \textit{cuCtxSynchronize} synchronizes a GPU task to wait for the
completion of GPU kernel;
(vi) \textit{cuMemcpyDtoH} transfers the data back to the host memory
from the device memory; and
(vii) \textit{cuMemFree}, \textit{cuModuleUnload}, and \textit{cuCtxDestroy} release the allocated memory space and the GPU context.

\SUBSECTION{GPU Scheduling}
Resource management problems have been addressed in many types of
real-time OSes (RTOSes)~\cite{spring,redline,itron,rk}.
In particular, those in Linux-based RTOSes have received considerable
attention~\cite{litmus,prk,rtai,yodaiken1999rtlinux,kato2009loadable}.
GPUs are also supported in Linux.
This work assumes that the design concept and function of OS are based
on Linux, such as LKM.

To meet real-time requirements in shared resource environments, such as
multi-core CPUs and GPUs, RTOSes should provide \textit{scheduling} and
\textit{resource reservation} functions.
Rate Monotonic (RM) and Earliest Deadline First (EDF)~\cite{sched:ll})
are well-known algorithms of priority-based scheduling for real-time
systems.
There are also many variants for resource reservation algorithms.
Examples include Constant Bandwidth Server (CBS)~\cite{rr:cbs} and Total
Bandwidth Server (TBS)~\cite{rr:tbs2}.

GPU computing must deal with data transfer bandwidth and compute cores
as a shared resource.
Therefore, scheduling and resource reservation must also be considered
for GPUs as well as CPUs in real-time systems.
TimeGraph and Gdev are involved with GPU resource management problems
but are not much aware of the fact that GPU tasks consume CPU time to
drive APIs.
To support GPUs in real-time systems, the OS scheduler must be able to
manage GPU tasks together with CPU tasks on the host side, while taking
care of GPU time for the device side.
Therefore, we consider the problem of CPU scheduling and GPU scheduling
with resource reservation mechanisms in a coordinated manner.

The recently developed GPUSync system supports CPU scheduling and GPU
scheduling.
In GPUSync, the device driver and runtime library for GPU computing are
proprietary black-box modules released by GPU vendors.
On the other hand, TimeGraph and Gdev address this problem by using
reverse-engineered open-source software.
Both approaches are limited to some extent, respectively.
Using black-box modules makes it difficult to manage the system in a
fine-grained manner.
Open-source software tends to lack some functionality due to
incompletion of reverse engineering.
GPUSync managed to incorporate black-box modules in scheduling and
resource reservation by arbitrating interrupt handlers and runtime
accesses.
This GPUSync approach is preferred in a sense that we can utilize
reliable proprietary driver and library, while we can still provide
scheduling and resource reservation functions.

\textbf{GPU Synchronization:}
Given that the GPU is a coprocessor connected to the host CPU,
synchronization between the GPU and the CPU must be considered to
guarantee the correctness of execution logics.
Most GPU architectures support two synchronization mechanisms.
One is based on memory-mapped registers called FENCE.
To use FENCE, we send special commands to the GPU when a GPU kernel is
launched so that the GPU can write the specified value to this
memory-mapped space when the GPU kernel is completed. 
On the host side, a GPU task is polling to monitor this value via the
mapped space.
The other technique is interrupt-based synchronization called NOTIFY.
To use NOTIFY, we also send special commands to the GPU similar to
FENCE.
Instead of writing to memory-mapped registers, NOTIFY raises the
interrupt from the GPU to the CPU and at the same time writes some
associated values to I/O registers of the GPU.
On the host side, a GPU task is suspended to wait for the occurrence of
interrupt.
NOTIFY allows other tasks to use CPU resources when the GPU task is
waiting for completion of its GPU kernel, though the scheduling overhead
is involved.
FENCE is easier to use and is more responsive than NOTIFY but is
implemented at the expense of CPU utilization.
More details about GPU architectures can be found in previous
work~\cite{kato:timegraph, kato:gdev, fujii:apsys2013}.

Gdev supports both NOTIFY and FENCE to synchronize the CPU and the GPU.
NOTIFY is primarily used for scheduling of GPU tasks, while FENCE is
used in driver-level synchronization. 
In Gdev, the implementation of GPU synchronization involves additional
commands to the GPU as aforementioned, which requires some modification
to the device driver.
GPUSync indirectly utilizes the NOTIFY technique with tasklet
interception~\cite{elliott2012robust} on top of the proprietary
black-box modules.
Tasklet refers to Linux's soft-irq implementation.
GPUSync identifies the interrupt that has invoked a GPU kernel using a
callback pointer with a tasklet.

\textbf{Loadable Kernel Modules:}
The main concept of our system is to enable both the CPU and GPU to be
managed by the OS scheduler without any code changes to the OS kernel
and device drivers.
CPU scheduling has already been demonstrated by
RESCH~\cite{kato2009loadable, asberg2012exsched} under this constraint.
In order to schedule GPU tasks, we must be able to hook the scheduling
points where the preceding GPU kernel is completed and the active
context is switched to the next GPU kernel.
The scheduling points can be hooked by two methods.
The API-driven method presented by RGEM~\cite{kato:rgem} explicitly
awakens the scheduler after GPU synchronization invoked by the API, such
as $cuCtxSynchronize()$.
The interrupt-driven method presented by TimeGraph and Gdev, on the
other hand, uses interrupts that can be configured by NOTIFY.
GPUSync is also based on this interrupt-driven method.
Especially in CUDA, the standard $cuCtxSynchronize()$ API synchronizes
completion of all GPU kernels.
Therefore, the API-driven method can be used if a GPU context issues no
more than one GPU kernel.
In other words, if a GPU task invokes multiple GPU kernels, the
interrupt-driven method is more approapriate to realize real-time
capabilities.

The interrupt-driven method forced TimeGraph, Gdev, and GPUSync to
modify the code of either the Linux kernel or device drivers.
This is because the interrupt service routine (ISR) must be managed to
create the scheduling points.
Gdev has developed independent synchronization mechanisms on top of the
proprietary software; however, Gdev still needs some modification to the
Linux kernel for scheduling and resource reservation.
As a result, the available release versions of the Linux kernel and
device drivers for TimeGraph, Gdev, and GPUSync are very limited.
A core challenge of this paper is to develop independent synchronization
mechanisms that do not require any modification to the OS kernel and
device drivers so that we can utilize the coordinated CPU and GPU
resource management scheme with a wide range of the Linux kernel and
device drivers. 

\textbf{Scope and Limitation:}
GPU resource management is involved with non-preemptive nature.
For example, the execution of GPU kernels is non-preemptive.
The data transfer between the CPU and the GPU is also non-preemptive.
Some previous work have addressed the problem of response time by
preventing overrun that occurs while dividing the
kernel~\cite{basaran:preemptive,sparc}.
The most difficult problem is the scheduling of self-suspending tasks
because the GPU is treated as an I/O device in the system. 
For example, GPU tasks are suspending until their GPU kernels are
completed.
The self-suspending problem was presented as a NP-HARD problem in
previous work~\cite{self-sus:1,self-sus:2}.
There are a lot of ongoing work on the scheduling of self-suspending
tasks~\cite{chattopadhyay2014limited,kim2013segment}.

Such a schedulability problem of GPU scheduling is not the scope of this
paper.
We rather focus on the design and implementation of GPU scheduling and
resource reservation with existing algorithms.
In this paper, we also restrict our attention to time resources but not
memory resources.
Our prototype system is limited to the Linux system and the CUDA
environment, but the concept of our method is applicable to other OSes
and programming languages, as far as they support the aforementioned
NOTIFY mechanism.
